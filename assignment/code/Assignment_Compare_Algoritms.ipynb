{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02944396",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9652bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from files\n",
    "from degree_freedom_queen import *\n",
    "from degree_freedom_king1 import *\n",
    "from degree_freedom_king2 import *\n",
    "from generate_game import *\n",
    "from Chess_env import *\n",
    "\n",
    "from neural_net import *\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bceca7c",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "\n",
    "You can find the environment in the file Chess_env, which contains the class Chess_env. To define an object, you need to provide the board size considered as input. In our example, size_board=4. \n",
    "Chess_env is composed by the following methods:\n",
    "\n",
    "1. Initialise_game. The method initialises an episode by placing the three pieces considered (Agent's king and queen, enemy's king) in the chess board. The outputs of the method are described below in order.\n",
    "\n",
    "     - S $\\;$ A matrix representing the board locations filled with 4 numbers: 0, no piece in that position; 1, location of the \n",
    "     agent's king; 2 location of the queen; 3 location of the enemy king.\n",
    "     \n",
    "     - X $\\;$ The features, that is the input to the neural network. See the assignment for more information regarding the            definition of the features adopted. To personalise this, go into the Features method of the class Chess_env() and change        accordingly.\n",
    "     \n",
    "     - allowed_a $\\;$ The allowed actions that the agent can make. The agent is moving a king, with a total number of 8                possible actions, and a queen, with a total number of $(board_{size}-1)\\times 8$ actions. The total number of possible actions correspond      to the sum of the two, but not all actions are allowed in a given position (movements to locations outside the borders or      against chess rules). Thus, the variable allowed_a is a vector that is one (zero) for an action that the agent can (can't)      make. Be careful, apply the policy considered on the actions that are allowed only.\n",
    "     \n",
    "\n",
    "2. OneStep. The method performs a one step update of the system. Given as input the action selected by the agent, it updates the chess board by performing that action and the response of the enemy king (which is a random allowed action in the settings considered). The first three outputs are the same as for the Initialise_game method, but the variables are computed for the position reached after the update of the system. The fourth and fifth outputs are:\n",
    "\n",
    "     - R $\\;$ The reward. To change this, look at the OneStep method of the class where the rewards are set.\n",
    "     \n",
    "     - Done $\\;$ A variable that is 1 if the episode has ended (checkmate or draw).\n",
    "     \n",
    "     \n",
    "3. Features. Given the chessboard position, the method computes the features.\n",
    "\n",
    "This information and a quick analysis of the class should be all you need to get going. The other functions that the class exploits are uncommented and constitute an example on how not to write a python code. You can take a look at them if you want, but it is not necessary.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9593a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITIALISE THE ENVIRONMENT\n",
    "\n",
    "size_board = 4\n",
    "env=Chess_Env(size_board)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332936f9",
   "metadata": {},
   "source": [
    "# Initialize game and set fixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece20429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALISE THE PARAMETERS OF YOUR NEURAL NETWORK AND...\n",
    "# PLEASE CONSIDER TO USE A MASK OF ONE FOR THE ACTION MADE AND ZERO OTHERWISE IF YOU ARE NOT USING VANILLA GRADIENT DESCENT...\n",
    "# WE SUGGEST A NETWORK WITH ONE HIDDEN LAYER WITH SIZE 200. \n",
    "\n",
    "\n",
    "S,X,allowed_a=env.Initialise_game()\n",
    "\n",
    "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
    "N_in=np.shape(X)[0]    ## INPUT SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1379dc9",
   "metadata": {},
   "source": [
    "# Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6e32f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from: models/qlearning_relu_None_seed_21\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'randn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_300770/1067207734.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqlearning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"qlearning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"seed_21\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msarsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sarsa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"seed_21\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dqn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"seed_21\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UZH/2022_spring/IntroRL/assignment/code/neural_net.py\u001b[0m in \u001b[0;36mload_from\u001b[0;34m(method, act_f_1, act_f_2, name_extension)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;31m# initialize neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m     \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_f_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_f_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;31m# network weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UZH/2022_spring/IntroRL/assignment/code/neural_net.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, N_in, N_h, N_a, activation_function_1, activation_function_2, method, seed, capacity, C)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# self.W1 = rng.random.randn(self.K+1, self.D+1)/np.sqrt(self.D+1)  # standard normal distribution, shape: (K+1, D+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# glorot/xavier normal initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# standard normal distribution, shape: (K+1, D+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;31m# self.W1 = rng.random.randn(self.K+1, self.D+1)  # standard normal distribution, shape: (K+1, D+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'randn'"
     ]
    }
   ],
   "source": [
    "qlearning = load_from(\"qlearning\", \"relu\", None, name_extension=\"seed_21\")\n",
    "sarsa = load_from(\"sarsa\", \"relu\", None, name_extension=\"seed_21\")\n",
    "dqn = load_from(\"dqn\", \"relu\", None, name_extension=\"seed_21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "ax.plot(exponential_moving_average(qlearning.R_history)[0], label=\"Q-Learning\")\n",
    "ax.plot(exponential_moving_average(sarsa.R_history)[0], label=\"SARSA\")\n",
    "ax.plot(exponential_moving_average(dqn.R_history)[0], label=\"DQN\")\n",
    "ax.set_xlabel(\"Episodes\")\n",
    "ax.set_ylabel(\"Reward\")\n",
    "# ax.set_ylim(.75, 1)\n",
    "ax.legend()\n",
    "ax.set_title(\"EMA of Rewards per Episode\")\n",
    "plt.savefig(\"../figures/ema_rewards_per_episode_comparison.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c13a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "ax.plot(exponential_moving_average(qlearning.N_moves_history)[0], label=\"Q-Learning\")\n",
    "ax.plot(exponential_moving_average(sarsa.N_moves_history)[0], label=\"SARSA\")\n",
    "ax.plot(exponential_moving_average(dqn.N_moves_history)[0], label=\"DQN\")\n",
    "ax.set_xlabel(\"Episodes\")\n",
    "ax.set_ylabel(\"Number of moves\")\n",
    "# ax.set_ylim(0, 18)\n",
    "ax.legend()\n",
    "ax.set_title(\"EMA of Number of Moves per Episode\")\n",
    "plt.savefig(\"../figures/ema_number_of_moves_per_episode_comparison.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f4dce",
   "metadata": {},
   "source": [
    "# Analyze Averaged Statistics over Multiple Runs (without Seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_rewards(method):\n",
    "    R_mean, R_std, N_moves_mean, N_moves_std = load_avg_statistics(method)\n",
    "\n",
    "    ema = exponential_moving_average(R_mean)[0]\n",
    "    R_std = exponential_moving_average(R_std)[0]\n",
    "\n",
    "    lower_bound = np.maximum(ema - R_std, 0)\n",
    "    upper_bound = np.minimum(ema + R_std, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "    ax.plot(exponential_moving_average(R_mean)[0], label=\"Average Reward\")\n",
    "    ax.fill_between(np.arange(len(R_mean)), lower_bound, upper_bound, alpha=0.2, label=\"1 Sigma Confidence Interval\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_avg_n_moves(method):\n",
    "    R_mean, R_std, N_moves_mean, N_moves_std = load_avg_statistics(method)\n",
    "\n",
    "    ema = exponential_moving_average(N_moves_mean)[0]\n",
    "    N_moves_std = exponential_moving_average(N_moves_std)[0]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    plt.plot(exponential_moving_average(N_moves_mean)[0], label=\"Average Number of Moves\")\n",
    "    plt.fill_between(np.arange(len(N_moves_mean)), ema-N_moves_std, ema+N_moves_std, alpha=0.2, label=\"95% Confidence Interval\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_avg_rewards(\"qlearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a06b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(R_mean, label=\"Q-Learning\")\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
