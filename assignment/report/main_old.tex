\documentclass[11pt, a4paper, twocolumn]{article}
\usepackage[english]{babel}

\usepackage{amsfonts} % use AMS math fonts
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}






\begin{document}



% --------------------------------------------------------------
% titlepage

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}

        \textbf{Chess Assignment}

        \vspace{0.5cm}
        Introduction to Reinforcement Learning

        \vspace{1.5cm}

        \textbf{Laurin van den Bergh}\\
        Matriculation number: 16-744-401

        \vfill

        Prof. Eleni Vasilaki\\
        Dr. Luca Manneschi\\
        \vspace{0.8cm}

        Department of Informatics\\
        University of Zurich\\
        Switzerland\\
        31.03.2022

    \end{center}
\end{titlepage}




\section{Introduction}

\section{Methods}

\section{Results}


\textbf{Question Answers}

\begin{itemize}
    \item[1.] explain algos: Q-learning and SARSA are both temporal-difference (TD) methods for learning Q-values from interaction with an environment. Both can be used for any policy and both try to attribute future rewards to previous actions, these (not actually the rewards get discounted, but the q-value) get discounted with the hyper parameter $\alpha\gamma$. \\
    differences: (they behave differently: cliff example) Both algorithms choose their first action according to their chosen policy, e.g. $\epsilon$-greedy. However, they differ in how they choose their second action which is then also used to update the Q-value of the first action (i.e. to compare the second action Q-value?). SARSA is an on-policy algorithm which means that it chooses the second action according to the same policy as it used for choosing the first action. Q-learning, on the other hand, chooses the second action greedily, i.e. it chooses the action for which it thinks is best at that point in time/training. \\
    advantages/disadvantages \\
    q learning taking optimal path, but sarsa taking riskier path. what does that mean for chess?
    shaping rewards changes the objective.

    \item[2. (group only)]
    \item[3.]
    \item[4.]  plot beta and gamma against performance metrics (maybe 3d)
    \item[5.] 
    \item[6. (group only)] 
    \item[7. (group only)] 
\end{itemize}


\section{Conclusion}



\section{Appendix}











    
\end{document}